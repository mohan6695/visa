#!/usr/bin/env python3
"""
Ollama MCP Server for KiloCode Integration
Provides Qwen model access through MCP protocol
"""

import asyncio
import json
import logging
from typing import Any, Optional
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ollama-mcp-server")

app = Server("ollama-mcp-server")

# Ollama configuration
OLLAMA_BASE_URL = "http://localhost:11434"

@app.list_tools()
async def list_tools() -> list[Tool]:
    """List available tools from Ollama"""
    return [
        Tool(
            name="ollama_chat",
            description="Chat with Qwen model using Ollama",
            inputSchema={
                "type": "object",
                "properties": {
                    "message": {
                        "type": "string",
                        "description": "The user's message to send to the model"
                    },
                    "system_prompt": {
                        "type": "string",
                        "description": "Optional system prompt to set context",
                        "default": "You are a helpful AI assistant."
                    },
                    "temperature": {
                        "type": "number",
                        "description": "Temperature for response generation (0.0-1.0)",
                        "default": 0.7
                    }
                },
                "required": ["message"]
            }
        ),
        Tool(
            name="ollama_embeddings",
            description="Generate embeddings using Ollama with nomic-embed-text",
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "Text to generate embeddings for"
                    }
                },
                "required": ["text"]
            }
        ),
        Tool(
            name="ollama_list_models",
            description="List available models in Ollama",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="ollama_generate",
            description="Generate text using Ollama with any model",
            inputSchema={
                "type": "object",
                "properties": {
                    "prompt": {
                        "type": "string",
                        "description": "The prompt to generate from"
                    },
                    "model": {
                        "type": "string",
                        "description": "Model to use (default: qwen2.5:3b)",
                        "default": "qwen2.5:3b"
                    },
                    "stream": {
                        "type": "boolean",
                        "description": "Whether to stream the response",
                        "default": False
                    }
                },
                "required": ["prompt"]
            }
        )
    ]

async def call_ollama_api(endpoint: str, data: dict) -> dict:
    """Make API call to Ollama"""
    import aiohttp
    
    url = f"{OLLAMA_BASE_URL}{endpoint}"
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=data) as response:
                return await response.json()
    except Exception as e:
        logger.error(f"Ollama API error: {e}")
        raise

@app.call_tool()
async def call_tool(name: str, arguments: Any) -> list[TextContent]:
    """Handle tool calls"""
    if name == "ollama_chat":
        message = arguments["message"]
        system_prompt = arguments.get("system_prompt", "You are a helpful AI assistant.")
        temperature = arguments.get("temperature", 0.7)
        
        data = {
            "model": "qwen2.5:3b",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": message}
            ],
            "stream": False,
            "options": {"temperature": temperature}
        }
        
        result = await call_ollama_api("/api/chat", data)
        response = result.get("message", {}).get("content", "No response")
        return [TextContent(type="text", text=response)]
    
    elif name == "ollama_embeddings":
        text = arguments["text"]
        data = {
            "model": "nomic-embed-text:latest",
            "input": text
        }
        
        result = await call_ollama_api("/api/embeddings", data)
        embedding = result.get("embedding", [])
        return [TextContent(type="text", text=json.dumps({"embedding": embedding, "dimensions": len(embedding)}))]
    
    elif name == "ollama_list_models":
        result = await call_ollama_api("/api/tags", {})
        models = result.get("models", [])
        return [TextContent(type="text", text=json.dumps({"models": models}, indent=2))]
    
    elif name == "ollama_generate":
        prompt = arguments["prompt"]
        model = arguments.get("model", "qwen2.5:3b")
        
        data = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        
        result = await call_ollama_api("/api/generate", data)
        response = result.get("response", "No response")
        return [TextContent(type="text", text=response)]
    
    return [TextContent(type="text", text=f"Unknown tool: {name}")]

async def main():
    """Main entry point"""
    async with stdio_server() as (read_stream, write_stream):
        await app.run(read_stream, write_stream, app.create_initialization_options())

if __name__ == "__main__":
    asyncio.run(main())
