#!/usr/bin/env python3
"""
Supabase Visa Posts Uploader with Chunking
Processes Facebook visa community posts and uploads to Supabase with semantic search
"""

import json
import os
import sys
import time
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import hashlib

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('supabase_upload.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

try:
    import openai
    from supabase import create_client, Client
    import numpy as np
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import re
    import html
except ImportError as e:
    logger.error(f"Missing required package: {e}")
    logger.info("Please install: pip install openai supabase scikit-learn numpy")
    sys.exit(1)

class SupabaseVisaPostsUploader:
    def __init__(self, supabase_url: str, supabase_key: str):
        """Initialize the uploader with Supabase client"""
        self.supabase: Client = create_client(supabase_url, supabase_key)
        self.openai_client = openai.OpenAI()
        
        # Configuration
        self.chunk_size = 50  # Process 50 posts at a time
        self.max_retries = 3
        self.retry_delay = 5  # seconds
        self.batch_size = 10  # Upload 10 posts per batch
        
        # Pre-defined data
        self.community_id = "550e8400-e29b-41d4-a716-446655440001"  # H1B Visa Community
        self.usa_country_id = 1  # United States
        self.embedding_model = "text-embedding-3-small"
        
        # Initialize TF-IDF vectorizer for fallback similarity
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1536,
            stop_words='english',
            ngram_range=(1, 2)
        )
        
        logger.info("Supabase Visa Posts Uploader initialized")
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text content"""
        if not text:
            return ""
        
        # Remove HTML entities and decode
        text = html.unescape(text)
        
        # Remove extra whitespace and newlines
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)]', ' ', text)
        
        # Limit length to avoid embedding issues
        text = text.strip()[:8000]  # Leave room for processing
        
        return text
    
    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate embedding for text using OpenAI"""
        if not text.strip():
            return None
            
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text[:8000]  # Limit input length
            )
            return response.data[0].embedding
        except Exception as e:
            logger.warning(f"Failed to generate embedding: {e}")
            return None
    
    def generate_tfidf_vector(self, texts: List[str]) -> Optional[np.ndarray]:
        """Generate TF-IDF vectors for a batch of texts"""
        try:
            if not texts or len([t for t in texts if t.strip()]) == 0:
                return None
            
            # Clean texts
            cleaned_texts = [self.clean_text(text) for text in texts]
            cleaned_texts = [t for t in cleaned_texts if t.strip()]
            
            if not cleaned_texts:
                return None
            
            # Fit and transform
            vectors = self.tfidf_vectorizer.fit_transform(cleaned_texts)
            return vectors.toarray()
        except Exception as e:
            logger.warning(f"Failed to generate TF-IDF vectors: {e}")
            return None
    
    def extract_tags_from_content(self, content: str, title: str = "") -> List[str]:
        """Extract relevant tags from post content"""
        if not content:
            return []
        
        text = f"{title} {content}".lower()
        
        # Define tag patterns
        tag_patterns = {
            'h1b': [r'\bh1b\b', r'h-1b', r'h1-b'],
            'h4': [r'\bh4\b', r'h-4', r'h4-ead', r'h4 ead'],
            'ead': [r'\bead\b', r'employment authorization'],
            'stamping': [r'stamping', r'visa stamp', r'consulate'],
            '221g': [r'221g', r'221 g', r'admin.*process'],
            'rfe': [r'\brfe\b', r'request.*evidence'],
            'interview': [r'interview', r'consulate.*interview'],
            'refusal': [r'refused', r'rejection', r'denied'],
            'employment': [r'employment', r'job', r'work'],
            'family': [r'family', r'spouse', r'dependent'],
            'travel': [r'travel', r're-?entry', r'port.*entry'],
            'documentation': [r'document', r'paperwork', r'form'],
            'timeline': [r'timeline', r'processing.*time', r'how long'],
            'attorney': [r'attorney', r'lawyer', r'legal']
        }
        
        found_tags = []
        for tag, patterns in tag_patterns.items():
            if any(re.search(pattern, text) for pattern in patterns):
                found_tags.append(tag)
        
        return found_tags
    
    def create_title_from_content(self, content: str) -> str:
        """Create a title from post content if none exists"""
        if not content:
            return "Visa Discussion"
        
        # Take first sentence or first 100 characters
        sentences = content.split('.')
        first_part = sentences[0] if sentences else content
        
        title = first_part.strip()[:100]
        
        # If it's too short or looks like a question, keep as is
        if len(title) < 20 or '?' in title:
            return title if title else "Visa Discussion"
        
        # Add ellipsis if truncated
        if len(first_part) > 100:
            title += "..."
        
        return title
    
    def process_facebook_post(self, fb_post: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Process a single Facebook post into our schema"""
        try:
            # Extract basic information
            external_id = fb_post.get('id', '')
            content = fb_post.get('text', '')
            url = fb_post.get('url', '')
            created_at_str = fb_post.get('time', '')
            
            if not external_id or not content:
                logger.warning(f"Skipping post missing external_id or content: {external_id}")
                return None
            
            # Parse timestamp
            try:
                if created_at_str:
                    external_created_at = datetime.fromisoformat(
                        created_at_str.replace('Z', '+00:00')
                    )
                else:
                    external_created_at = datetime.now(timezone.utc)
            except:
                external_created_at = datetime.now(timezone.utc)
            
            # Create user from Facebook user data
            fb_user = fb_post.get('user', {})
            user_display_name = fb_user.get('name', 'Anonymous User')
            user_id = f"fb_{fb_user.get('id', 'anonymous')}"
            
            # Create title from content
            title = self.create_title_from_content(content)
            
            # Determine post type based on content
            content_lower = content.lower()
            if any(word in content_lower for word in ['experience', 'went', 'got', 'received']):
                post_type = 'experience'
            elif any(word in content_lower for word in ['update', 'status', 'approved', 'rejected']):
                post_type = 'update'
            else:
                post_type = 'question'
            
            # Extract tags
            tags = self.extract_tags_from_content(content, title)
            
            processed_post = {
                'external_id': external_id,
                'title': title,
                'content': content,
                'content_html': self.clean_text(content),
                'author_id': user_id,
                'community_id': self.community_id,
                'country_id': self.usa_country_id,
                'url': url,
                'post_type': post_type,
                'upvotes': fb_post.get('likesCount', 0),
                'comment_count': fb_post.get('commentsCount', 0),
                'external_created_at': external_created_at.isoformat(),
                'source_platform': 'facebook',
                'source_data': fb_post,
                'extracted_tags': tags
            }
            
            return processed_post
            
        except Exception as e:
            logger.error(f"Error processing Facebook post {fb_post.get('id', 'unknown')}: {e}")
            return None
    
    def upload_batch(self, posts: List[Dict[str, Any]]) -> bool:
        """Upload a batch of posts to Supabase"""
        if not posts:
            return True
        
        try:
            # Generate embeddings for the batch
            texts = [post.get('content', '') for post in posts]
            embeddings = [self.generate_embedding(text) for text in texts]
            
            # Update posts with embeddings
            for i, post in enumerate(posts):
                post['embedding'] = embeddings[i]
                # Remove extracted_tags as it's for processing only
                if 'extracted_tags' in post:
                    del post['extracted_tags']
            
            # Upload posts in smaller batches
            for i in range(0, len(posts), self.batch_size):
                batch = posts[i:i + self.batch_size]
                
                # Insert into Supabase
                result = self.supabase.table('posts').upsert(batch, on_conflict='external_id').execute()
                
                if hasattr(result, 'error') and result.error:
                    logger.error(f"Supabase error: {result.error}")
                    return False
                
                logger.info(f"Uploaded batch {i//self.batch_size + 1}: {len(batch)} posts")
                time.sleep(0.5)  # Small delay between batches
            
            return True
            
        except Exception as e:
            logger.error(f"Error uploading batch: {e}")
            return False
    
    def process_and_upload_posts(self, json_file_path: str) -> bool:
        """Process Facebook posts and upload to Supabase with chunking"""
        logger.info(f"Starting to process and upload posts from {json_file_path}")
        
        # Load Facebook posts data
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                facebook_posts = json.load(f)
            logger.info(f"Loaded {len(facebook_posts)} Facebook posts")
        except Exception as e:
            logger.error(f"Failed to load JSON file: {e}")
            return False
        
        # Process posts in chunks
        total_processed = 0
        total_uploaded = 0
        
        for i in range(0, len(facebook_posts), self.chunk_size):
            chunk = facebook_posts[i:i + self.chunk_size]
            logger.info(f"Processing chunk {i//self.chunk_size + 1}/{(len(facebook_posts) + self.chunk_size - 1)//self.chunk_size}")
            
            # Process each post in the chunk
            processed_posts = []
            for fb_post in chunk:
                processed_post = self.process_facebook_post(fb_post)
                if processed_post:
                    processed_posts.append(processed_post)
            
            # Upload the processed chunk
            if processed_posts:
                for attempt in range(self.max_retries):
                    try:
                        if self.upload_batch(processed_posts):
                            total_uploaded += len(processed_posts)
                            logger.info(f"Successfully uploaded {len(processed_posts)} posts")
                            break
                        else:
                            logger.warning(f"Upload attempt {attempt + 1} failed")
                            if attempt < self.max_retries - 1:
                                time.sleep(self.retry_delay * (attempt + 1))
                    except Exception as e:
                        logger.error(f"Upload attempt {attempt + 1} failed with exception: {e}")
                        if attempt < self.max_retries - 1:
                            time.sleep(self.retry_delay * (attempt + 1))
            else:
                logger.warning("No valid posts in this chunk")
            
            total_processed += len(chunk)
            logger.info(f"Progress: {total_processed}/{len(facebook_posts)} posts processed, {total_uploaded} uploaded")
            
            # Small delay between chunks to avoid rate limiting
            if i + self.chunk_size < len(facebook_posts):
                time.sleep(1)
        
        logger.info(f"Upload complete: {total_uploaded}/{total_processed} posts successfully uploaded")
        return total_uploaded > 0
    
    def create_sample_users(self):
        """Create sample users for the community"""
        try:
            # Check if users already exist
            result = self.supabase.table('users').select('id').limit(1).execute()
            if result.data:
                logger.info("Users already exist, skipping creation")
                return
            
            # Create anonymous user for Facebook posts
            anonymous_user = {
                'id': 'fb_anonymous',
                'display_name': 'Facebook User',
                'is_verified': False
            }
            
            result = self.supabase.table('users').upsert(anonymous_user, on_conflict='id').execute()
            if hasattr(result, 'error') and result.error:
                logger.error(f"Failed to create anonymous user: {result.error}")
            else:
                logger.info("Created anonymous Facebook user")
                
        except Exception as e:
            logger.error(f"Error creating sample users: {e}")

def main():
    """Main function"""
    # Supabase configuration
    SUPABASE_URL = os.getenv('SUPABASE_URL', 'your-supabase-url')
    SUPABASE_KEY = os.getenv('SUPABASE_KEY', 'your-supabase-key')
    
    if SUPABASE_URL == 'your-supabase-url' or SUPABASE_KEY == 'your-supabase-key':
        logger.error("Please set SUPABASE_URL and SUPABASE_KEY environment variables")
        sys.exit(1)
    
    # Initialize uploader
    uploader = SupabaseVisaPostsUploader(SUPABASE_URL, SUPABASE_KEY)
    
    # Create sample users
    uploader.create_sample_users()
    
    # Process and upload posts
    json_file_path = "../Downloads/new_posts.json"
    
    if not os.path.exists(json_file_path):
        logger.error(f"JSON file not found: {json_file_path}")
        sys.exit(1)
    
    success = uploader.process_and_upload_posts(json_file_path)
    
    if success:
        logger.info("✅ Successfully uploaded visa posts to Supabase!")
        logger.info("You can now access the Stack Overflow style interface at http://localhost:9000")
    else:
        logger.error("❌ Failed to upload posts to Supabase")
        sys.exit(1)

if __name__ == "__main__":
    main()